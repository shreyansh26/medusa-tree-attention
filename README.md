# Medusa Tree Attention Mechanism - Standalone Implementation

This is a standalone implementation of the core components required for the **Tree Attention** mechanism used in the Medusa framework for accelerating Large Language Model (LLM) inference. 
Most of the code is taken from the [original Medusa implementation](https://github.com/FasterDecoding/Medusa). This is mainly just for my own understanding and reference. I have added comments and added some notes here in the README to aid my understanding.

## Overview

Medusa enhances LLM inference speed by predicting multiple future tokens in parallel using additional "Medusa heads". Instead of just predicting token `t+1`, it predicts candidates for `t+2`, `t+3`, etc., based on the state at token `t`.

These candidate tokens are structured as a tree, where each path from the root represents a potential continuation sequence. To verify these candidate sequences efficiently, Medusa processes the unique tokens involved across all considered paths in a *single* parallel forward pass through the transformer model.

**Tree Attention** is the specialized attention mechanism used during this parallel verification step. It ensures that each token being processed only attends to:
1.  The original input prefix (tokens `0` to `t`).
2.  Itself.
3.  Its valid ancestors within the *specific candidate path* it belongs to, as defined by the pre-configured `medusa_choices`.

This script implements the functions necessary to:
*   Generate the standard causal attention mask.
*   Generate the specific buffers (`medusa_attn_mask`, `tree_indices`, `medusa_position_ids`, `retrieve_indices`) that encode the Medusa tree structure and facilitate the parallel processing and subsequent evaluation.
*   Combine the base causal mask and the Medusa tree structure mask into the final attention mask used during the parallel verification step.

## Core Concepts

*   **Medusa Heads:** Additional prediction heads attached to the base LLM, each responsible for predicting candidates for a specific future time step (Head 0 for `t+2`, Head 1 for `t+3`, etc.).
*   **Candidate Paths (`medusa_choices`):** A predefined list of tuples defining the tree structure. Each tuple `(c_0, c_1, ..., c_{k-1})` represents a path of length `k+1` (`[base_pred, cand_t+2, ..., cand_t+k+1]`), where `c_i` indicates using the `c_i`-th best prediction (0-indexed rank) from Medusa Head `i`.
*   **Parallel Verification:** Processing all unique nodes (tokens) from the selected `medusa_choices` paths simultaneously in one forward pass.
*   **Tree Attention Mask:** A specialized attention mask applied during parallel verification. It combines standard causal masking with constraints derived from the `medusa_choices` tree structure, ensuring information flows only along valid paths.
*   **Buffers:** Pre-computed tensors (masks, indices) generated once based on `medusa_choices` to avoid redundant calculations during the inference loop.

## File Contents

*   **`TOPK` (Constant):** Represents the number of top candidates considered from each Medusa head during candidate generation. Used in calculating `tree_indices`.
*   **`pad_path` (Helper Function):** Utility to pad lists, used in generating `retrieve_indices`.
*   **`_make_causal_mask` (Function):** Generates a standard additive causal attention mask (`0.0` allows, `-inf` blocks). It ensures that new tokens can attend to the `past_key_values_length` (prefix) and causally attend among themselves.
*   **`generate_medusa_buffers` (Function):** The main function to generate all necessary pre-computed buffers based on `medusa_choices`. Returns a dictionary containing the buffers described below.
*   **`apply_medusa_tree_attention` (Function):** Takes the `base_causal_mask` (generated by `_make_causal_mask` for the current step) and the pre-computed `medusa_attn_mask` buffer, and combines them to produce the final additive attention mask used in the transformer layers during parallel verification.
*   **`if __name__ == "__main__":` Block:** A runnable example demonstrating the usage of the functions and showing the generated buffers and the final combined attention mask.

## Key Buffers Explained

The `generate_medusa_buffers` function produces the following crucial tensors:

1.  **`medusa_attn_mask`**:
    *   **Shape:** `(1, 1, medusa_len, medusa_len)`
    *   **Format:** Additive mask (`0.0` allows attention, `-inf` blocks attention).
    *   **Purpose:** Encodes the *tree structure constraints* only. It defines which pairs of nodes *within the Medusa candidate block* are allowed to attend to each other based on the ancestor relationships defined in `medusa_choices`. A node `i` can attend to node `j` if `j` is the root (index 0), `j` is `i` itself, or `j` is an ancestor of `i` in the tree.
    *   **Usage:** Combined with the `base_causal_mask` in `apply_medusa_tree_attention` to create the final mask.

2.  **`tree_indices`**:
    *   **Shape:** `(medusa_len,)`
    *   **Format:** Long tensor containing indices.
    *   **Purpose:** Maps each position (node) in the parallel processing sequence (length `medusa_len`) to an index in a *flattened candidate token list*. This flattened list (conceptual) contains `[base_pred_token, head0_top1, ..., head0_topK, head1_top1, ..., head1_topK, ...]`. `tree_indices` selects the appropriate token ID from this flattened list to form the `tree_candidates` tensor (the actual input to the parallel verification step).
    *   **Calculation:** `tree_indices[k]` (for node `k > 0` corresponding to `path = sorted_choices[k-1]`) = `path[-1] + TOPK * head_idx + 1`, where `head_idx` is the index of the Medusa head responsible for the *last* token in `path`.
    *   **Usage:** Used in `generate_candidates` (from the original Medusa codebase, not shown in this script) to construct the `tree_candidates` input tensor for the parallel step.

3.  **`medusa_position_ids`**:
    *   **Shape:** `(medusa_len,)`
    *   **Format:** Long tensor containing position indices.
    *   **Purpose:** Provides relative positional information for the nodes processed in parallel. Standard sequential position IDs are not sufficient because the nodes represent different future time steps.
    *   **Calculation:** Root (index 0) gets position 0. Nodes corresponding to paths of length 1 (Head 0 outputs) get position 1. Nodes for paths of length 2 (Head 1 outputs) get position 2, and so on.
    *   **Usage:** Fed as `position_ids` to the transformer model during the parallel verification step (`tree_decoding` in the original codebase) to allow correct application of relative positional embeddings (like RoPE).

4.  **`retrieve_indices`**:
    *   **Shape:** `(num_unique_paths, max_path_len + 1)`
    *   **Format:** Long tensor containing indices.
    *   **Purpose:** Reconstructs the full candidate paths from the output of the parallel verification step. The model's output logits (`tree_logits`) are ordered according to the parallel sequence (length `medusa_len`). `retrieve_indices` acts as a gather map to reassemble these logits into rows, where each row corresponds to one of the original candidate paths defined by `medusa_choices`.
    *   **Calculation:** Identifies unique paths, finds the node indices corresponding to each token along the path (Root + Node indices), pads them, and prepends a `0` for the Root state.
    *   **Usage:** Used on the output logits (`tree_logits`) of the parallel step to prepare them for evaluation (`evaluate_posterior` in the original codebase). Also used in `generate_candidates` to create `cart_candidates`.

5.  **`sorted_medusa_choices`**:
    *   **Format:** List[Tuple[int, ...]]
    *   **Purpose:** The sorted version of the input `medusa_choices`, used internally for consistent indexing. Returned for potential reference or debugging.

## How it Works (Workflow during Inference)

1.  **Initialization (Once):**
    *   Define the desired tree structure (`medusa_choices`).
    *   Call `generate_medusa_buffers` to pre-compute all the necessary buffers.

2.  **Decoding Loop (Each Step):**
    *   **(Not in this script)** Generate a flat list of candidate tokens (`candidates`) using the base model's last logit and the Medusa heads' predictions.
    *   **(Not in this script)** Use the `tree_indices` buffer to select tokens from `candidates` and form the `tree_candidates` tensor (shape `(batch_size, medusa_len)`). This is the input sequence for the parallel step.
    *   Determine the current `prefix_len` (length of the sequence processed so far).
    *   Call `_make_causal_mask` to get the `base_causal_mask` for the new `medusa_len` tokens relative to the `prefix_len`.
    *   Call `apply_medusa_tree_attention`, passing the `base_causal_mask` and the pre-computed `medusa_attn_mask` buffer (which is additive). This yields the `final_attention_mask`.
    *   **(Not in this script)** Execute the transformer model's forward pass using `tree_candidates`, the `final_attention_mask`, and the `medusa_position_ids` buffer. This produces `tree_logits`.
    *   **(Not in this script)** Use the `retrieve_indices` buffer to gather logits from `tree_logits` into path-ordered sequences.
    *   **(Not in this script)** Evaluate these path-ordered logits (e.g., using `evaluate_posterior`) to determine the `best_candidate` path and its `accept_length`.
    *   **(Not in this script)** Update the main input sequence, KV cache, and other states based on the accepted path (`update_inference_inputs`).

## Running the Example

To run the included demonstration:

```bash
python tree_attention.py
```

The output will show:     
* The simulation parameters.     
* The generated Medusa Tree Attention Mask (showing tree constraints).     
* The generated Tree Indices.    
* The generated Medusa Position IDs.    
* The generated Retrieve Indices.    
* The Base Causal Attention Mask for the example step.    
* The Final Combined Attention Mask incorporating both causal and tree constraints.     
* A verification check for a specific node, comparing the calculated final mask row against the expected pattern based on the combined logic.     

## Dependencies

PyTorch (torch)


## Limitations / Context
This script focuses only on the generation of the Medusa buffers and the creation of the final tree attention mask. It does not include:

* The Medusa model architecture itself (multiple heads).     
* Candidate token generation from model logits.     
* The actual model forward pass using the generated mask.     
* Candidate evaluation logic (evaluate_posterior).     
* KV cache management or state updates (update_inference_inputs). 


It serves as a tool to understand and verify the specific attention masking mechanism employed by Medusa during its parallel decoding phase.